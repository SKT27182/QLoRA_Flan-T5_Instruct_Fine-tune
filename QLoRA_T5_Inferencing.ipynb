{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Environment Setup","metadata":{}},{"cell_type":"code","source":"import sys\n\nif 'kaggle_web_client' in sys.modules:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    HUGGINGFACE_API_KEY = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n    WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\nelse:\n    from dotenv import load_dotenv\n    import os\n    load_dotenv()\n    HUGGINGFACE_API_KEY = os.getenv('HUGGINGFACE_API_KEY')\n    WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:24:25.085828Z","iopub.execute_input":"2023-07-23T10:24:25.086211Z","iopub.status.idle":"2023-07-23T10:24:25.516048Z","shell.execute_reply.started":"2023-07-23T10:24:25.086178Z","shell.execute_reply":"2023-07-23T10:24:25.515011Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Installing Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q accelerate evaluate rouge_score\n!pip install -q datasets loralib einops\n!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:24:25.527171Z","iopub.execute_input":"2023-07-23T10:24:25.527809Z","iopub.status.idle":"2023-07-23T10:26:23.843439Z","shell.execute_reply.started":"2023-07-23T10:24:25.527770Z","shell.execute_reply":"2023-07-23T10:26:23.842150Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Huggingface login","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nfrom huggingface_hub import login\nlogin(token=HUGGINGFACE_API_KEY)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-23T10:26:23.848013Z","iopub.execute_input":"2023-07-23T10:26:23.848324Z","iopub.status.idle":"2023-07-23T10:26:24.004765Z","shell.execute_reply.started":"2023-07-23T10:26:23.848295Z","shell.execute_reply":"2023-07-23T10:26:24.003846Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# imports","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\nimport evaluate\n\nimport torch\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n\nimport bitsandbytes as bnb\nfrom peft import PeftModel, PeftConfig\n\n\n# from transformers import DataCollatorWithPadding, DataCollatorForLanguageModeling\n# from transformers import T5ForConditionalGeneration, T5TokenizerFast\nfrom transformers import AutoTokenizer, get_scheduler, BitsAndBytesConfig, GenerationConfig\nfrom transformers import  AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n# from transformers import  AutoModelForCausalLM","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:26:24.006334Z","iopub.execute_input":"2023-07-23T10:26:24.006682Z","iopub.status.idle":"2023-07-23T10:26:38.315154Z","shell.execute_reply.started":"2023-07-23T10:26:24.006649Z","shell.execute_reply":"2023-07-23T10:26:38.314176Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:26:38.317275Z","iopub.execute_input":"2023-07-23T10:26:38.317647Z","iopub.status.idle":"2023-07-23T10:26:38.326124Z","shell.execute_reply.started":"2023-07-23T10:26:38.317612Z","shell.execute_reply":"2023-07-23T10:26:38.324490Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data, tokenizer and Peft fine-tunned model","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"SKT27182/Preprocessed_OpenOrca\", streaming=True, use_auth_token=True)\n# samples = []\nn_samples = 1000\n\ndata = {\"id\":[], \"system_prompt\":[], \"question\":[], \"response\": [], \"length_before_preprocessing\":[]}\n\nfor i, sample in enumerate(dataset[\"train\"]):\n    if i >= n_samples:\n        break\n    for key, value in sample.items():\n        data[key].append(value)\n        \nopen_orca = Dataset.from_dict(data)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:36:30.764749Z","iopub.execute_input":"2023-07-23T10:36:30.765132Z","iopub.status.idle":"2023-07-23T10:36:34.512399Z","shell.execute_reply.started":"2023-07-23T10:36:30.765101Z","shell.execute_reply":"2023-07-23T10:36:34.511401Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# split the data for training and validation purpose\nopen_orca = open_orca.train_test_split(train_size=0.9, seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:36:50.854467Z","iopub.execute_input":"2023-07-23T10:36:50.854978Z","iopub.status.idle":"2023-07-23T10:36:50.866909Z","shell.execute_reply.started":"2023-07-23T10:36:50.854940Z","shell.execute_reply":"2023-07-23T10:36:50.865929Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"peft_model_id = \"shirsh10mall/First_LLM_Project\"\n# peft_model_id = \"SKT27182/Qlora\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained( config.base_model_name_or_path, return_dict=True, load_in_8bit=True, \n                                                 device_map={\"\":0}, trust_remote_code=True, )\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n# tokenizer.pad_token = tokenizer.eos_token\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:34:04.083974Z","iopub.execute_input":"2023-07-23T10:34:04.084372Z","iopub.status.idle":"2023-07-23T10:34:13.862204Z","shell.execute_reply.started":"2023-07-23T10:34:04.084340Z","shell.execute_reply":"2023-07-23T10:34:13.861068Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading adapter_model.bin:   0%|          | 0.00/19.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc82b32272549d48ba088ec93146526"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): T5ForConditionalGeneration(\n      (shared): Embedding(32128, 1024)\n      (encoder): T5Stack(\n        (embed_tokens): Embedding(32128, 1024)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): T5Stack(\n        (embed_tokens): Embedding(32128, 1024)\n        (block): ModuleList(\n          (0): T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x T5Block(\n            (layer): ModuleList(\n              (0): T5LayerSelfAttention(\n                (SelfAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): T5LayerCrossAttention(\n                (EncDecAttention): T5Attention(\n                  (q): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                  (v): Linear8bitLt(\n                    in_features=1024, out_features=1024, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): T5LayerFF(\n                (DenseReluDense): T5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): T5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): T5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Zero shot Prediction\n","metadata":{}},{"cell_type":"code","source":"def analyse_zero_shot_model(data, indx, tokenizer, model, peft=False):\n    prompt = data[indx][\"system_prompt\"]\n    question = data[indx][\"question\"]\n    response = data[indx][\"response\"]\n    print(\"Prompt:\")\n    print(prompt)\n    print()\n\n    print('Question:')\n    print(question)\n    print()\n    \n    print(\"Response\")\n    print(response)\n    print()\n    \n    tokenized_input = tokenizer(prompt, question, padding=True, truncation=True, return_tensors=\"pt\")\n    \n    print(\"Tokenized Input: Prompt + Question\")\n    print(tokenized_input)\n    print()\n    \n    print(tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"][0]))\n    print()\n    \n    print(\"Zero-shot Prediction:\")\n    \n    device = model.device\n    \n    if peft:\n    \n        predicted_response = model.generate(input_ids = tokenized_input[\"input_ids\"].to(device), generation_config=GenerationConfig(max_new_tokens=200, num_beams=50))\n    else:\n        predicted_response = model.generate(input_ids = tokenized_input[\"input_ids\"].to(device))\n    \n    \n    predicted_output = tokenizer.decode(predicted_response[0], skip_special_tokens=True)\n    print(predicted_output)\n    \n    \nindex = 11\nanalyse_zero_shot_model(open_orca[\"train\"], index, tokenizer, model, peft=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:39:23.028040Z","iopub.execute_input":"2023-07-23T10:39:23.028441Z","iopub.status.idle":"2023-07-23T10:39:58.700130Z","shell.execute_reply.started":"2023-07-23T10:39:23.028408Z","shell.execute_reply":"2023-07-23T10:39:58.698404Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Prompt:\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n\nQuestion:\nTranslate the following sentence to Russian:\nWould you completely bet that it couldn't usurp Google, given time?\n\nResponse\nTo translate the given sentence into Russian, I'll analyze the sentence structure and words, identify the Russian counterparts, and then construct the Russian sentence while following the correct grammar and syntax rules. \n\nEnglish sentence: Would you completely bet that it couldn't usurp Google, given time?\n\nTranslation process:\n- \"Would you\" is a question construction that, in Russian, translates as \"Вы бы\" or \"Ты бы\" (the formal and informal ways to address \"you,\" respectively; I'll go with the formal version)\n- \"completely bet\" can be translated as \"полностью поспорили\"\n- \"that\" as a conjunction introducing a clause can be translated as \"что\"\n- \"it\" can be translated as \"он\" or \"она\" (masculine and feminine; in this case, I'll use the neuter form for an object or idea: \"оно\")\n- \"couldn't\" is a contraction of \"could not,\" which translates as \"не смогло бы\"\n- \"usurp\" can be translated as \"свергнуть\" or \"заменить\"; I'll go with \" заменить,\" which means \"replace\" and captures the idea of gaining power or supremacy\n- \"Google\" remains the same since it's a proper noun\n- \"given time\" can be translated as \"со временем\"\n\nNow, I'll construct the Russian sentence while following correct grammar and syntax rules.\n\nRussian sentence: Вы бы полностью поспорили, что оно не смогло бы заменить Google со временем?\n\nThe sentence has now been translated into Russian.\n\nTokenized Input: Prompt + Question\n{'input_ids': tensor([[  148,    33,    46,  7833,  6165,     5,  6674,    56,    25,   428,\n            25,     3,     9,  2491,     5,   696,  1288,    19,    12,   743,\n             8,  2491,    38, 13855,   120,    38,    25,    54,     5,   818,\n          5505,     8,  2491,   317,  1147,    18,   969,    18,  7910,    11,\n         18686,    39,  2245,     5,     1, 30355,    15,     8,   826,  7142,\n            12,  4263,    10,  5328,    25,  1551,    36,    17,    24,    34,\n          2654,    31,    17,   178,   450,   102,  1163,     6,   787,    97,\n            58,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n['▁You', '▁are', '▁an', '▁AI', '▁assistant', '.', '▁User', '▁will', '▁you', '▁give', '▁you', '▁', 'a', '▁task', '.', '▁Your', '▁goal', '▁is', '▁to', '▁complete', '▁the', '▁task', '▁as', '▁faithful', 'ly', '▁as', '▁you', '▁can', '.', '▁While', '▁performing', '▁the', '▁task', '▁think', '▁step', '-', 'by', '-', 'step', '▁and', '▁justify', '▁your', '▁steps', '.', '</s>', '▁Translat', 'e', '▁the', '▁following', '▁sentence', '▁to', '▁Russian', ':', '▁Would', '▁you', '▁completely', '▁be', 't', '▁that', '▁it', '▁couldn', \"'\", 't', '▁us', 'ur', 'p', '▁Google', ',', '▁given', '▁time', '?', '</s>']\n\nZero-shot Prediction:\n                                                                                                   \n","output_type":"stream"}]},{"cell_type":"code","source":"index = 110\nanalyse_zero_shot_model(open_orca[\"train\"], index, tokenizer, model, peft=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:41:48.761830Z","iopub.execute_input":"2023-07-23T10:41:48.762196Z","iopub.status.idle":"2023-07-23T10:42:24.688670Z","shell.execute_reply.started":"2023-07-23T10:41:48.762168Z","shell.execute_reply":"2023-07-23T10:42:24.687457Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Prompt:\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n\nQuestion:\nHere is a review left by a customer on a product. Would you say he was satisfied or dissatisfied? Title: Michelle Bachman? Please Review: This book was made to make money off of others. WHat in the heck does michelle bachmann know?\nA:\n\nResponse\nStep 1: Analyze the title of the review\nThe title \"Michelle Bachman?\" suggests that the customer might be doubtful or confused about the subject matter, indicating negative emotions.\n\nStep 2: Analyze the review content\nIn the review, the customer mentions that the book was made to make money off of others and questions what Michelle Bachman knows. This showcases dissatisfaction and disappointment with the book.\n\nStep 3: Determine overall satisfaction\nBased on the negative tone and questioning in both the title and the content of the review, I would say the customer is dissatisfied with the product.\n\nTokenized Input: Prompt + Question\n{'input_ids': tensor([[  148,    33,    46,  7833,  6165,     5,  6674,    56,    25,   428,\n            25,     3,     9,  2491,     5,   696,  1288,    19,    12,   743,\n             8,  2491,    38, 13855,   120,    38,    25,    54,     5,   818,\n          5505,     8,  2491,   317,  1147,    18,   969,    18,  7910,    11,\n         18686,    39,  2245,     5,     1,   947,    19,     3,     9,  1132,\n           646,    57,     3,     9,   884,    30,     3,     9,   556,     5,\n          5328,    25,   497,     3,    88,    47,  7035,    42,  1028,  9275,\n             7,  8549,    26,    58, 11029,    10, 15275, 16453,   348,    58,\n           863,  4543,    10,   100,   484,    47,   263,    12,   143,   540,\n           326,    13,   717,     5,     3, 15313,   144,    16,     8, 24783,\n           405,  2278,   693,     3,  6425,  2434,   214,    58,    71,    10,\n             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n['▁You', '▁are', '▁an', '▁AI', '▁assistant', '.', '▁User', '▁will', '▁you', '▁give', '▁you', '▁', 'a', '▁task', '.', '▁Your', '▁goal', '▁is', '▁to', '▁complete', '▁the', '▁task', '▁as', '▁faithful', 'ly', '▁as', '▁you', '▁can', '.', '▁While', '▁performing', '▁the', '▁task', '▁think', '▁step', '-', 'by', '-', 'step', '▁and', '▁justify', '▁your', '▁steps', '.', '</s>', '▁Here', '▁is', '▁', 'a', '▁review', '▁left', '▁by', '▁', 'a', '▁customer', '▁on', '▁', 'a', '▁product', '.', '▁Would', '▁you', '▁say', '▁', 'he', '▁was', '▁satisfied', '▁or', '▁dis', 'sati', 's', 'fie', 'd', '?', '▁Title', ':', '▁Michelle', '▁Bach', 'man', '?', '▁Please', '▁Review', ':', '▁This', '▁book', '▁was', '▁made', '▁to', '▁make', '▁money', '▁off', '▁of', '▁others', '.', '▁', 'WH', 'at', '▁in', '▁the', '▁heck', '▁does', '▁mich', 'elle', '▁', 'bach', 'mann', '▁know', '?', '▁A', ':', '</s>']\n\nZero-shot Prediction:\nThis review was left by a customer on a product. This book was made to make money off of others. What in the heck does michelle bachmann know? This review was left by a customer on a product. This book was made to make money off of others. What in the heck does michelle bachmann know? This review was left by a customer on a product. This book was made to make money off of others. What in the heck does michelle bachmann know? This review was left by a customer on a product. This book was made to make money off of others. What in the heck does michelle bachmann know? This review was left by a customer on a product. This book was made to make money off of others. What in the heck does michelle bachmann know? This review was left by a customer on a product. This book was made to make money\n","output_type":"stream"}]},{"cell_type":"code","source":"index = 899\nanalyse_zero_shot_model(open_orca[\"train\"], index, tokenizer, model, peft=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:43:29.234995Z","iopub.execute_input":"2023-07-23T10:43:29.235581Z","iopub.status.idle":"2023-07-23T10:44:05.435011Z","shell.execute_reply.started":"2023-07-23T10:43:29.235538Z","shell.execute_reply":"2023-07-23T10:44:05.433842Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Prompt:\nYou are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.\n\nQuestion:\n\nQ: I am verifying the answers generated by an automatic system to the following question: how deep can be drill for deep underwater Suggested answer: Oil spills result in oil sinking to the bottom of the ocean smothering plant life leaving fish without food and shelter to survive. Should I validate this answer?\nA:\n\nResponse\nNo, the suggested answer does not appropriately respond to the question. The question is asking for the depth that can be drilled underwater, while the provided answer discusses the impact of oil spills in the ocean.\n\nA more suitable answer would be: The depth that can be drilled underwater mainly depends on the technology being used. Currently, the most advanced deepwater drilling operations typically reach depths of 7,500 to 12,000 feet (2,300 to 3,660 meters) under the ocean surface. Ultra-deepwater drilling operations, on the other hand, can extend beyond 12,000 feet (3,660 meters) in depth. However, the actual depth might vary due to factors such as the drilling rig's capabilities, equipment, and geological conditions.\n\nTokenized Input: Prompt + Question\n{'input_ids': tensor([[  148,    33,    46,  7833,  6165,     5,  7740,     3,     9,  3117,\n          1525,    78,  1139,   278,    22,    17,   174,    12,   960,  1067,\n            12,   734,     8,  1525,     5,     1,  1593,    10,    27,   183,\n         10446,    53,     8,  4269,  6126,    57,    46,  6569,   358,    12,\n             8,   826,   822,    10,   149,  1659,    54,    36,  9722,    21,\n          1659, 22409,   180, 13917,    15,  6265,  1525,    10,  6067, 15487,\n             7,   741,    16,  1043,  5067,    53,    12,     8,  2007,    13,\n             8,  5431,     3,     7, 21226,    53,  1475,   280,  3140,  2495,\n           406,   542,    11,  8596,    12,  7905,     5,  5066,    27, 16742,\n            48,  1525,    58,    71,    10,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n['▁You', '▁are', '▁an', '▁AI', '▁assistant', '.', '▁Provide', '▁', 'a', '▁detailed', '▁answer', '▁so', '▁user', '▁don', '’', 't', '▁need', '▁to', '▁search', '▁outside', '▁to', '▁understand', '▁the', '▁answer', '.', '</s>', '▁Q', ':', '▁I', '▁am', '▁verify', 'ing', '▁the', '▁answers', '▁generated', '▁by', '▁an', '▁automatic', '▁system', '▁to', '▁the', '▁following', '▁question', ':', '▁how', '▁deep', '▁can', '▁be', '▁drill', '▁for', '▁deep', '▁underwater', '▁S', 'ugg', 'e', 'sted', '▁answer', ':', '▁Oil', '▁spill', 's', '▁result', '▁in', '▁oil', '▁sink', 'ing', '▁to', '▁the', '▁bottom', '▁of', '▁the', '▁ocean', '▁', 's', 'mother', 'ing', '▁plant', '▁life', '▁leaving', '▁fish', '▁without', '▁food', '▁and', '▁shelter', '▁to', '▁survive', '.', '▁Should', '▁I', '▁validate', '▁this', '▁answer', '?', '▁A', ':', '</s>']\n\nZero-shot Prediction:\nOil spills result in oil sinking to the bottom of the ocean smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering plant life leaving fish without food and shelter to survive smothering\n","output_type":"stream"}]},{"cell_type":"code","source":"index = 655\nanalyse_zero_shot_model(open_orca[\"train\"], index, tokenizer, model, peft=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:44:05.437164Z","iopub.execute_input":"2023-07-23T10:44:05.437550Z","iopub.status.idle":"2023-07-23T10:44:41.027857Z","shell.execute_reply.started":"2023-07-23T10:44:05.437514Z","shell.execute_reply":"2023-07-23T10:44:41.026782Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Prompt:\nYou are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n\nQuestion:\nWhat most naturally follows?\n\nA small group of people are seen playing lacrosse on a field running up and down the field and pushing one another. several shots of players hitting one another\n\nResponse\nStep 1: Identify the context\nSince the scenario describes a group of people playing lacrosse, it is clear that the context is a sports match or practice session. Lacrosse is a physically demanding and contact-intensive game.\n\nStep 2: Understand the specific details\nThe description mentions that players are running up and down the field, pushing and hitting one another. This indicates that the players are actively engaged in the game and are using physical contact as allowed by the rules to gain an advantage.\n\nStep 3: Determine a logical continuation\nBased on the context and specific details, it seems natural to expect that the players will continue their gameplay with similar intensity. They would attempt to score by advancing the ball down the field and shooting it into the opposing team's goal.\n\nStep 4: Formulate the continuation\nA possible continuation could be: The intensity of the game continues to grow as both teams fight for control of the ball. Players execute skilled maneuvers to evade their opponents, while defenders use aggressive tactics to regain possession. The crowd watches intently, cheering for their favorite team as they witness impressive displays of athleticism on the field.\n\nStep 5: Justify the steps\nThe chosen continuation focuses on the ongoing gameplay in the lacrosse match. By highlighting an increase in intensity, it logically expands on the existing scenario of players hitting and pushing one another. The continuation also considers the overall objective of the game – scoring goals – and includes the reactions of a potential audience.\n\nTokenized Input: Prompt + Question\n{'input_ids': tensor([[  148,    33,    46,  7833,  6165,     5,  6674,    56,    25,   428,\n            25,     3,     9,  2491,     5,   696,  1288,    19,    12,   743,\n             8,  2491,    38, 13855,   120,    38,    25,    54,     5,   818,\n          5505,     8,  2491,   317,  1147,    18,   969,    18,  7910,    11,\n         18686,    39,  2245,     5,     1,   363,   167,  6212,  6963,    58,\n            71,   422,   563,    13,   151,    33,   894,  1556,    50, 11465,\n            15,    30,     3,     9,  1057,  1180,    95,    11,   323,     8,\n          1057,    11,  9759,    80,   430,     5,   633,  6562,    13,  1508,\n         10849,    80,   430,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n['▁You', '▁are', '▁an', '▁AI', '▁assistant', '.', '▁User', '▁will', '▁you', '▁give', '▁you', '▁', 'a', '▁task', '.', '▁Your', '▁goal', '▁is', '▁to', '▁complete', '▁the', '▁task', '▁as', '▁faithful', 'ly', '▁as', '▁you', '▁can', '.', '▁While', '▁performing', '▁the', '▁task', '▁think', '▁step', '-', 'by', '-', 'step', '▁and', '▁justify', '▁your', '▁steps', '.', '</s>', '▁What', '▁most', '▁naturally', '▁follows', '?', '▁A', '▁small', '▁group', '▁of', '▁people', '▁are', '▁seen', '▁playing', '▁la', 'cross', 'e', '▁on', '▁', 'a', '▁field', '▁running', '▁up', '▁and', '▁down', '▁the', '▁field', '▁and', '▁pushing', '▁one', '▁another', '.', '▁several', '▁shots', '▁of', '▁players', '▁hitting', '▁one', '▁another', '</s>']\n\nZero-shot Prediction:\nA small group of people are seen playing lacrosse on a field running up and down the field and pushing one another. Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step: Step-by-step:\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}}]}